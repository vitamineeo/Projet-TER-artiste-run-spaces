import pandas as pd
import numpy as np
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import umap
import hdbscan
from sklearn.feature_extraction.text import CountVectorizer
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from umap.umap_ import UMAP
from hdbscan import HDBSCAN
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
pio.renderers.default = "notebook"
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sentence_transformers import SentenceTransformer
import numpy as np
import re


# Chargement du fichier XLSX
file_path = "fichier_traduit.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')

# Afficher les premières lignes pour vérifier
df.head()

def clean_text(text):
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Nettoyer les marqueurs de retour à la ligne spécifiques
    text = text.replace('_*x000d*_', ' ')
    
    # Nettoyer les espaces multiples
    text = ' '.join(text.split())
    
    return text.strip()

def combine_columns(row):
    """Combine les colonnes de texte pour un espace donné sans les labels."""
    combined_text = []
    for content_type in ['activites', 'presentation', 'historique', 'réponse1', 'réponse2']:
        content = row[content_type]
        if pd.notna(content) and str(content).strip() != "":
            cleaned_text = clean_text(content)
            combined_text.append(cleaned_text)
    return " ".join(combined_text)  # Fusionner sans séparateur explicite


# Combiner les colonnes de texte pour chaque espace
df['texte_combine'] = df.apply(combine_columns, axis=1)
df = df[['nom', 'texte_combine']]

# Afficher quelques exemples de texte combiné
print("\nExemples de texte combiné :")
print(df[['nom', 'texte_combine']].head(10))

df[['nom', 'texte_combine']].to_csv("output.csv", index=False, encoding='utf-8')
print("Fichier 'output.csv' généré.")

import pandas as pd

def clean_text(text):
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Nettoyer les marqueurs de retour à la ligne spécifiques
    text = text.replace('_*x000d*_', ' ')
    
    # Nettoyer les espaces multiples
    text = ' '.join(text.split())
    
    return text.strip()

def load_and_prepare_data(df):
    processed_texts = []
    space_names = []
    
    for idx, row in df.iterrows():
        content = row['texte_combine']  # Utilisation directe de la colonne fusionnée
        if pd.notna(content) and str(content).strip() != "":
            # Nettoyer le texte
            cleaned_text = clean_text(content)
            
            # Vérifier si le texte n'est pas vide après nettoyage
            if cleaned_text:
                processed_texts.append(cleaned_text)
                space_names.append(str(row['nom']).strip())

    # Créer le DataFrame
    processed_df = pd.DataFrame({
        'nom': space_names,
        'texte': processed_texts
    })
    
    # Afficher les statistiques
    print("Statistiques :")
    print(f"Nombre total de documents : {len(processed_df)}")
    print("\nNombre d'espaces uniques :", len(processed_df['nom'].unique()))
    
    # Afficher quelques exemples pour vérification
    print("\nExemples de textes nettoyés :")
    print(processed_df['texte'].iloc[0][:200])  # Premier extrait de texte
    
    return processed_df

# Préparer les données
processed_df = load_and_prepare_data(df)


# Section 3 : Configuration et Entraînement du Modèle
###########################################

def clean_word(word):
    # Nettoyer un mot individuel
    word = re.sub(r'x000D|x0002', '', word)
    word = word.replace('_', ' ')
    
    # Séparer les dates et les mots
    word = re.sub(r'(\d{4})([A-Za-z])', r'\1 \2', word)
    
    # Supprimer la ponctuation
    word = re.sub(r'[^\w\s]', ' ', word)
    
    # Nettoyer les espaces
    word = ' '.join(word.split())
    
    # Limiter la longueur à 2 mots maximum
    words = word.split()
    if len(words) > 2:
        words = words[:2]
    word = ' '.join(words)
    
    # Ne pas retourner de mots trop courts ou uniquement numériques
    if len(word) <= 2 or word.isdigit():
        return ""
        
    return word

def clean_topic_names(topic_model):
    cleaned_topics = {}
    for topic_id, topic_info in topic_model.get_topics().items():
        seen_words = set()
        cleaned_words = []
        
        for word, score in topic_info:
            cleaned_word = clean_word(word)
            
            # Vérifier la validité du mot
            if cleaned_word and cleaned_word.lower() not in seen_words:
                words_in_phrase = set(cleaned_word.lower().split())
                # Vérifier si les mots individuels n'ont pas déjà été utilisés
                if not words_in_phrase.intersection(seen_words):
                    cleaned_words.append((cleaned_word, score))
                    seen_words.update(words_in_phrase)
        
        # Garder uniquement les 5 meilleurs mots significatifs
        cleaned_topics[topic_id] = cleaned_words[:5]
    
    return cleaned_topics

def preprocess_text(text):
    # Nettoyer le texte
    text = re.sub(r'x000D|x0002', '', text)
    text = re.sub(r'_+', ' ', text)
    text = re.sub(r'(\d{4})([A-Za-z])', r'\1 \2', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    
    # Limiter la longueur des phrases
    words = text.split()
    phrases = [' '.join(words[i:i+2]) for i in range(0, len(words), 2)]
    
    return ' '.join(phrases).strip()

# Configuration des modèles
umap_model = UMAP(
    n_neighbors=15,
    n_components=5,
    min_dist=0.0,
    metric='cosine',
    random_state=42
)

hdbscan_model = HDBSCAN(
    min_cluster_size=3,
    min_samples=2,
    metric='euclidean',
    cluster_selection_method='eom',
    prediction_data=True
)

# Configuration du vectorizer avec nettoyage amélioré
vectorizer_model = CountVectorizer(
    stop_words=list(ENGLISH_STOP_WORDS),
    ngram_range=(1, 3),  # Limiter à 3-grams max
    preprocessor=preprocess_text
)

# Définir le modèle de phrase AVANT BERTopic
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Configuration et entraînement de BERTopic
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    min_topic_size=3,
    n_gram_range=(1, 2),
    nr_topics="auto",
    calculate_probabilities=True,
    verbose=True
)

# Entraînement
topics, probs = topic_model.fit_transform(processed_df['texte'].tolist())

# Nettoyer les topics après l'entraînement
cleaned_topics = clean_topic_names(topic_model)

# Vérifier si les embeddings de topics existent
if hasattr(topic_model, "topic_embeddings_") and topic_model.topic_embeddings_ is not None:
    # Obtenir les embeddings des topics
    topic_embeddings = topic_model.topic_embeddings_

    # Calculer la similarité cosinus entre topics
    topic_sim_matrix = cosine_similarity(topic_embeddings)

    # Afficher la heatmap des similarités entre topics

    plt.figure(figsize=(12, 8))
    sns.heatmap(topic_sim_matrix, cmap="coolwarm", xticklabels=False, yticklabels=False)
    plt.title("Matrice de similarité entre les topics")
    plt.show()
    

# Ajouter les résultats au DataFrame
processed_df['topic'] = topics

# Pour les probabilités
if isinstance(probs, np.ndarray):
    max_probs = np.max(probs, axis=1)
    processed_df['topic_probability'] = max_probs
else:
    processed_df['topic_probability'] = 1.0

# Afficher les topics nettoyés pour vérification
print("\nTopics nettoyés :")
for topic_id, words in cleaned_topics.items():
    if topic_id != -1:
        print(f"\nTopic {topic_id}:")
        print(", ".join([word for word, _ in words[:5]]))

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import pandas as pd

# 5. Analyse des relations entre topics
def analyze_topic_relationships(df, topic_model):
    # Créer une matrice de relation entre topics, en excluant le topic -1
    topic_matrix = df[df['topic'] != -1]['topic'].value_counts(normalize=True) * 100
    
    # Visualisation sous forme de barplot avec légende
    plt.figure(figsize=(12, 8))
    barplot = sns.barplot(x=topic_matrix.index, y=topic_matrix.values, palette='YlOrRd')
    
    # Changer la couleur de l'arrière-plan du graphique
    plt.gca().set_facecolor('#f0f0f0')  # Couleur gris clair pour l'arrière-plan
    
    # Ajouter un titre et labels
    plt.title('Distribution des Topics')
    plt.ylabel('Pourcentage (%)')
    plt.xlabel('Topic ID')

    # Affichage des pourcentages sur les barres du barplot
    for p in barplot.patches:
        barplot.annotate(f'{p.get_height():.2f}%', 
                         (p.get_x() + p.get_width() / 2., p.get_height()),
                         ha='center', va='center', 
                         fontsize=12, color='black', 
                         xytext=(0, 5), textcoords='offset points')
    
    # Afficher le graphique
    plt.tight_layout()
    plt.show()
    
    # Analyse détaillée des topics
    print("\nAnalyse détaillée des topics :")
    for topic in sorted(df['topic'].unique()):
        if topic != -1:
            print(f"\nTopic {topic}:")
            keywords = topic_model.get_topic(topic)
            print(f"Mots-clés: {', '.join([word for word, _ in keywords[:5]])}")
            
            # Afficher quelques exemples de documents
            print("Exemples de documents:")
            examples = df[df['topic'] == topic]['texte'].head(1)
            if not examples.empty:
                print(examples.iloc[0][:200] + "...")
                
def create_interactive_topic_mapping(df, topic_model):
    # Préparer les données pour le graphique interactif
    topic_data = []
    for topic in sorted(df['topic'].unique()):
        if topic != -1:
            keywords = topic_model.get_topic(topic)
            topic_docs = df[df['topic'] == topic]
            
            topic_data.append({
                'Topic': f"Topic {topic}",
                'Count': len(topic_docs),
                'Keywords': ', '.join([word for word, _ in keywords[:3]])
            })
    
    # Créer un DataFrame pour le graphique Sunburst
    topic_df = pd.DataFrame(topic_data)
    
    # Création du graphique Sunburst
    fig = px.sunburst(
        topic_df,
        path=['Topic'],
        values='Count',
        hover_data=['Keywords'],
        width=800,
        height=800
    )
    
    # Mise à jour du layout du graphique avec une légende
    fig.update_layout(
        title="Mapping des Topics",
        legend_title="Topic ID",
        legend=dict(
            x=0.85,
            y=0.85
        )
    )
    
    # Sauvegarde du graphique dans un fichier HTML
    fig.write_html("topic_mapping.html")
    return fig

# 6. Exécuter les analyses
analyze_topic_relationships(processed_df, topic_model)
fig = create_interactive_topic_mapping(processed_df, topic_model)
fig.show()

# 7. Sauvegarder les résultats dans un fichier Excel
detailed_analysis = pd.DataFrame({
    'topic': processed_df['topic'],
    'texte': processed_df['texte']
}).groupby('topic').agg({
    'texte': 'count'
})

detailed_analysis.to_excel('topic_analysis.xlsx')

# Section 4 : Visualisations Interactives
###########################################
# 1. Intertopic Distance Map
topic_viz = topic_model.visualize_topics()
topic_viz.show()
topic_viz.write_html("intertopic_distance_map.html")

# 2. Document Map
doc_viz = topic_model.visualize_documents(processed_df['texte'])
doc_viz.show()
doc_viz.write_html("document_visualization.html")

# 3. Topic Hierarchy
hierarchy_viz = topic_model.visualize_hierarchy()
hierarchy_viz.show()
hierarchy_viz.write_html("topic_hierarchy.html")

# 4. Topic Distribution
barchart_viz = topic_model.visualize_barchart()
barchart_viz.show()
barchart_viz.write_html("topic_distribution.html")



# Section 5 : Analyses par Type de Contenu
###########################################
# Analyse des topics par type
topics_series = pd.Series(processed_df['topic'].values, index=processed_df.index)

# Pour chaque type de contenu, analyse des topics dominants et des mots-clés
for content_type in processed_df['type_texte'].unique():
    print(f"\nAnalyse pour {content_type}:")
    mask = processed_df['type_texte'] == content_type
    subset_topics = topics_series[mask]
    
    print("Topics dominants:")
    print(subset_topics.value_counts().head())
    
    print("\nMots-clés des topics dominants:")
    for topic in subset_topics.value_counts().head().index:
        if topic != -1:
            keywords = topic_model.get_topic(topic)[:5]  # Affichage des 5 premiers mots-clés
            print(f"Topic {topic}: {', '.join([word for word, _ in keywords])}")

# Visualisation de la distribution des topics par type de texte
topic_by_type = pd.DataFrame({
    'type_texte': processed_df['type_texte'],
    'topic': processed_df['topic']
})

# Calcul du nombre de documents par type de texte et topic
topic_counts = topic_by_type.groupby(['type_texte', 'topic']).size().reset_index(name='count')

# Création du graphique avec les topics et le type de texte
plt.figure(figsize=(15, 8))
pivot_table = topic_counts.pivot(index='topic', columns='type_texte', values='count').fillna(0)
pivot_table.plot(kind='bar', stacked=True)

# Mise à jour des labels et du titre
plt.title("Distribution des topics par type de texte")
plt.xlabel("Topic")
plt.ylabel("Nombre de documents")
plt.legend(title="Type de texte")
plt.tight_layout()

# Affichage du graphique
plt.show()


# Section 6 : Analyse des Relations Sémantiques améliorée
###########################################
# Création du graphe sémantique
G = nx.Graph()
embeddings = topic_model._extract_embeddings(processed_df['texte'].tolist())
similarities = cosine_similarity(embeddings)

# Ajouter les nœuds
for i in range(len(processed_df)):
    G.add_node(i, 
               space_name=processed_df['nom'].iloc[i],
               content_type=processed_df['type_texte'].iloc[i],
               topic=topics[i])

# Ajouter les arêtes
threshold = 0.3
for i in range(len(processed_df)):
    for j in range(i+1, len(processed_df)):
        if similarities[i,j] > threshold:
            if (processed_df['nom'].iloc[i] == processed_df['nom'].iloc[j] or 
                processed_df['type_texte'].iloc[i] == processed_df['type_texte'].iloc[j]):
                G.add_edge(i, j, weight=similarities[i,j])

# Configuration du style de la visualisation
plt.style.use('default')  # Reset du style
plt.figure(figsize=(20, 12), facecolor='white')
ax = plt.gca()
ax.set_facecolor('white')

# Couleurs distinctes et plus visibles
content_colors = {
    'activites': '#1f77b4',      # Bleu
    'presentation': '#2ca02c',    # Vert
    'historique': '#d62728',      # Rouge
    'reponse1': '#9467bd',       # Violet
    'reponse2': '#ff7f0e'        # Orange
}

# Calcul de la disposition du graphe
pos = nx.spring_layout(G, k=1, iterations=50)

# Dessiner les arêtes en premier avec une couleur claire
nx.draw_networkx_edges(G, pos, 
                      alpha=0.2, 
                      edge_color='grey',
                      width=0.5)

# Dessiner les nœuds avec des couleurs plus visibles
for content_type, color in content_colors.items():
    nodes = [n for n, attr in G.nodes(data=True) 
            if attr['content_type'] == content_type]
    if nodes:
        nx.draw_networkx_nodes(G, pos, 
                             nodelist=nodes,
                             node_color=color,
                             node_size=150,
                             alpha=0.8,
                             label=content_type.capitalize())

# Amélioration de la légende
plt.legend(bbox_to_anchor=(1.05, 1),
          loc='upper left',
          title='Types de Contenu',
          title_fontsize=12,
          fontsize=10,
          facecolor='white',
          edgecolor='black',
          framealpha=1)

# Titre et ajustements
plt.title("Graphe des Relations Sémantiques entre Documents",
          pad=20,
          fontsize=14,
          fontweight='bold')

# Ajustement des marges pour bien voir la légende
plt.tight_layout()
plt.savefig('graphe_semantique.png', 
            dpi=300,
            bbox_inches='tight',
            facecolor='white')
plt.show()


# Section 7 : Annotations et Pipeline
###########################################
def generate_topic_annotations(topic_model, topics):
    """
    Génère des annotations descriptives pour chaque topic
    """
    annotations = {}
    for topic_id in set(topics):
        if topic_id != -1:
            # Obtenir les mots-clés du topic
            keywords = topic_model.get_topic(topic_id)
            # Générer une annotation basée sur les mots-clés principaux
            main_keywords = [word for word, _ in keywords[:3]]
            annotation = " & ".join(main_keywords)
            annotations[topic_id] = f"Thème {topic_id}: {annotation}"
    
    # Ajouter une annotation pour le topic -1 (outliers)
    annotations[-1] = "Autres thèmes"
    return annotations

def topic_annotation_pipeline(new_texts, topic_model, existing_annotations=None):
    """
    Pipeline pour appliquer les annotations à de nouveaux textes
    
    Parameters:
    -----------
    new_texts : list
        Liste des nouveaux textes à analyser
    topic_model : BERTopic
        Modèle BERTopic entraîné
    existing_annotations : dict, optional
        Annotations existantes à réutiliser
        
    Returns:
    --------
    DataFrame avec les résultats de l'analyse
    """
    # Transformer les nouveaux textes
    new_topics, new_probs = topic_model.transform(new_texts)
    
    # Utiliser les annotations existantes ou en générer de nouvelles
    if existing_annotations is None:
        annotations = generate_topic_annotations(topic_model, new_topics)
    else:
        annotations = existing_annotations
    
    # Créer un DataFrame avec les résultats
    results = pd.DataFrame({
        'texte': new_texts,
        'topic': new_topics,
        'annotation': [annotations.get(topic, "Autre") for topic in new_topics],
        'probabilite': [np.max(prob) for prob in new_probs]
    })
    
    return results

# Application des annotations au DataFrame existant
topic_annotations = generate_topic_annotations(topic_model, topics)
processed_df['annotation'] = processed_df['topic'].map(topic_annotations)

# Affichage des résultats
print("\nExemple d'annotations générées :")
for topic_id, annotation in list(topic_annotations.items())[:5]:
    print(f"{annotation}")

# Visualisation de la distribution des annotations
plt.figure(figsize=(15, 8), facecolor='white')
annotation_counts = processed_df['annotation'].value_counts()
colors = plt.cm.viridis(np.linspace(0, 1, len(annotation_counts)))

annotation_counts.plot(kind='bar', color=colors)
plt.title("Distribution des Annotations", fontsize=14, pad=20)
plt.xlabel("Annotations", fontsize=12)
plt.ylabel("Nombre de Documents", fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Section 8 : Export des Résultats Finaux
###########################################
# Sauvegarder les résultats enrichis
results_df = processed_df.copy()
results_df.to_excel("results_with_annotations.xlsx", index=False, engine='openpyxl')

# Afficher un résumé final
print("\nRésumé de l'analyse :")
print(f"Nombre total de documents : {len(results_df)}")
print(f"Nombre de topics identifiés : {len(topic_annotations)}")
print("\nAperçu des résultats finaux :")
display(results_df[['nom', 'type_texte', 'topic', 'annotation']].head())



