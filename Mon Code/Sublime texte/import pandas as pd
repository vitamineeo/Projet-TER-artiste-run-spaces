import pandas as pd
import numpy as np
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import umap
import hdbscan
from sklearn.feature_extraction.text import CountVectorizer
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from umap.umap_ import UMAP
from hdbscan import HDBSCAN
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
pio.renderers.default = "notebook"
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sentence_transformers import SentenceTransformer
import numpy as np
import re


# Chargement du fichier XLSX
file_path = "fichier_traduit.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')

# Afficher les premières lignes pour vérifier
df.head()

def clean_text(text):
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Nettoyer les marqueurs de retour à la ligne spécifiques
    text = text.replace('_*x000d*_', ' ')
    
    # Nettoyer les espaces multiples
    text = ' '.join(text.split())
    
    return text.strip()

def combine_columns(row):
    """Combine les colonnes de texte pour un espace donné sans les labels."""
    combined_text = []
    for content_type in ['activites', 'presentation', 'historique', 'réponse1', 'réponse2']:
        content = row[content_type]
        if pd.notna(content) and str(content).strip() != "":
            cleaned_text = clean_text(content)
            combined_text.append(cleaned_text)
    return " ".join(combined_text)  # Fusionner sans séparateur explicite


# Combiner les colonnes de texte pour chaque espace
df['texte_combine'] = df.apply(combine_columns, axis=1)
df = df[['nom', 'texte_combine']]

# Afficher quelques exemples de texte combiné
print("\nExemples de texte combiné :")
print(df[['nom', 'texte_combine']].head(10))

df[['nom', 'texte_combine']].to_csv("output.csv", index=False, encoding='utf-8')
print("Fichier 'output.csv' généré.")


def clean_text(text):
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Nettoyer les marqueurs de retour à la ligne spécifiques
    text = text.replace('_*x000d*_', ' ')
    
    # Nettoyer les espaces multiples
    text = ' '.join(text.split())
    
    return text.strip()

def load_and_prepare_data(df):
    processed_texts = []
    space_names = []
    
    for idx, row in df.iterrows():
        content = row['texte_combine']  # Utilisation directe de la colonne fusionnée
        if pd.notna(content) and str(content).strip() != "":
            # Nettoyer le texte
            cleaned_text = clean_text(content)
            
            # Vérifier si le texte n'est pas vide après nettoyage
            if cleaned_text:
                processed_texts.append(cleaned_text)
                space_names.append(str(row['nom']).strip())

    # Créer le DataFrame
    processed_df = pd.DataFrame({
        'nom': space_names,
        'texte': processed_texts
    })
    
    # Afficher les statistiques
    print("Statistiques :")
    print(f"Nombre total de documents : {len(processed_df)}")
    print("\nNombre d'espaces uniques :", len(processed_df['nom'].unique()))
    
    # Afficher quelques exemples pour vérification
    print("\nExemples de textes nettoyés :")
    print(processed_df['texte'].iloc[0][:200])  # Premier extrait de texte
    
    return processed_df

# Préparer les données
processed_df = load_and_prepare_data(df)

# Définir des stop words plus complets
custom_stop_words = list(ENGLISH_STOP_WORDS) + [
    # Articles et déterminants
    'the', 'a', 'an', 'les', 'la', 'le', 'les', 'des',
    
    # Conjonctions
    'and', 'or', 'but', 'et', 'ou', 'mais',
    
    # Prépositions
    'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
    'dans', 'sur', 'pour', 'de', 'par',
    
    # Pronoms
    'it', 'its', 'this', 'that', 'these', 'those',
    'il', 'elle', 'ils', 'elles', 'ce', 'cette', 'ces',
    
    # Verbes communs
    'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'est', 'sont', 'être',
    
    # Mots spécifiques au contexte
    'new', 'space', 'art', 'the', 'artists', 'artist', 'work',
    'exhibition', 'gallery', 'contemporary', 'artistic', 'association',
    
    # Caractères et abréviations
    'etc', 'eg', 'ie', 'cf',
    
    # Mots additionnels à filtrer
    'x000d', 'www', 'http', 'https',
    'article', 'articles',
    'extra', 'certain',
    'located', 'large', 'small',
    'old', 'new',
    'works', 'create',
    'time', 'year',
    'team', 'project',
    'place', 'site',
    'run', 'open'
]
def clean_word(word):
    # Nettoyer et mettre en minuscules
    word = word.lower()
    
    # Supprimer les URLs
    if 'http' in word or 'www' in word:
        return ""
    
    # Supprimer les caractères x000D plus efficacement
    word = re.sub(r'x000d|x0002', '', word, flags=re.IGNORECASE)
    
    # Supprimer les retours chariot et autres caractères spéciaux
    word = re.sub(r'[\r\n\t]', ' ', word)
    
    # Supprimer les underscores
    word = word.replace('_', ' ')
    
    # Supprimer les nombres isolés et les dates
    word = re.sub(r'\b\d+\b', '', word)
    word = re.sub(r'\b\d{4}\b', '', word)
    
    # Supprimer la ponctuation et les caractères spéciaux
    word = re.sub(r'[^\w\s]', ' ', word)
    
    # Nettoyer les espaces multiples
    word = ' '.join(word.split())
    
    # Ne garder que les mots significatifs
    if (word.lower() in custom_stop_words or 
        len(word) <= 2 or 
        any(char.isdigit() for char in word)):
        return ""
    
    return word

def clean_topic_names(topic_model):
    cleaned_topics = {}
    for topic_id, topic_info in topic_model.get_topics().items():
        seen_words = set()
        cleaned_words = []
        
        for word, score in topic_info:
            cleaned_word = clean_word(word)
            
            if cleaned_word and cleaned_word.lower() not in seen_words:
                words_in_phrase = set(cleaned_word.lower().split())
                if not words_in_phrase.intersection(seen_words):
                    if score > 0.01:  # Seuil minimum pour le score
                        cleaned_words.append((cleaned_word, score))
                        seen_words.update(words_in_phrase)
        
        cleaned_topics[topic_id] = cleaned_words[:5]
    
    return cleaned_topics

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    
    # Nettoyer le texte
    text = text.lower()
    
    # Supprimer les URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)
    
    # Nettoyer les caractères spéciaux
    text = re.sub(r'x000d|x0002', '', text, flags=re.IGNORECASE)
    text = re.sub(r'[\r\n\t]', ' ', text)
    text = re.sub(r'_+', ' ', text)
    
    # Supprimer les nombres et dates
    text = re.sub(r'\b\d+\b', '', text)
    text = re.sub(r'\b\d{4}\b', '', text)
    
    # Supprimer la ponctuation
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Nettoyer les espaces multiples
    text = re.sub(r'\s+', ' ', text)
    
    # Filtrer les stop words
    words = text.split()
    words = [w for w in words if w not in custom_stop_words]
    
    return ' '.join(words).strip()

# Configuration des modèles
umap_model = UMAP(
    n_neighbors=15,
    n_components=5,
    min_dist=0.0,
    metric='cosine',
    random_state=42
)

hdbscan_model = HDBSCAN(
    min_cluster_size=5,  # Augmenté de 3 à 5
    min_samples=3,       # Augmenté de 2 à 3
    metric='euclidean',
    cluster_selection_method='eom',
    prediction_data=True
)

# Configuration du vectorizer avec paramètres améliorés
vectorizer_model = CountVectorizer(
    stop_words=custom_stop_words,
    ngram_range=(1, 3),
    preprocessor=preprocess_text,
    min_df=3,    # Augmenté de 2 à 3
    max_df=0.9,  # Ignorer les termes qui apparaissent dans plus de 90% des documents
    token_pattern=r'(?u)\b[a-zA-Z]{3,}\b'  # Au moins 3 caractères
)

# Définir le modèle de phrase
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Configuration de BERTopic
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    min_topic_size=5,           # Augmenté de 3 à 5
    n_gram_range=(1, 3),        # Augmenté à (1,3)
    nr_topics="auto",           # Force un nombre spécifique de topics
    top_n_words=10,             # Augmenté pour avoir plus de mots par topic
    calculate_probabilities=True,
    verbose=True
)

# Vérification et nettoyage des données
if 'texte' not in processed_df.columns:
    raise ValueError("La colonne 'texte' n'existe pas dans le DataFrame")

# Filtrer les textes vides
processed_df = processed_df[processed_df['texte'].notna() & (processed_df['texte'].str.strip() != '')]

# Entraînement
topics, probs = topic_model.fit_transform(processed_df['texte'].tolist())

# Nettoyer les topics après l'entraînement
cleaned_topics = clean_topic_names(topic_model)

# Ajouter les résultats au DataFrame
processed_df['topic'] = topics
processed_df['topic_probability'] = np.max(probs, axis=1) if isinstance(probs, np.ndarray) else 1.0


# Afficher les topics nettoyés
# Dans votre cellule d'affichage, remplacez le code par :
print("\nTopics identifiés :")
for topic_id, words in cleaned_topics.items():
    if topic_id != -1:  # Exclure le topic -1 (outliers)
        print(f"\nTopic {topic_id}:")
        print(", ".join([word for word, _ in words]))  # On n'affiche que les mots

print(f"\nNombre total de topics : {len(cleaned_topics) - 1}")

# 5. Analyse des relations entre topics
def analyze_topic_relationships(df, topic_model):
    
    # Analyse détaillée des topics
    print("\nAnalyse détaillée des topics :")
    for topic in sorted(df['topic'].unique()):
        if topic != -1:
            print(f"\nTopic {topic}:")
            keywords = topic_model.get_topic(topic)
            print(f"Mots-clés: {', '.join([word for word, _ in keywords[:5]])}")
            
            # Afficher quelques exemples de documents
            print("Exemples de documents:")
            examples = df[df['topic'] == topic]['texte'].head(1)
            if not examples.empty:
                print(examples.iloc[0][:200] + "...")
                
def create_interactive_topic_mapping(df, topic_model):
    # Préparer les données pour le graphique interactif
    topic_data = []
    for topic in sorted(df['topic'].unique()):
        if topic != -1:
            keywords = topic_model.get_topic(topic)
            topic_docs = df[df['topic'] == topic]
            
            topic_data.append({
                'Topic': f"Topic {topic}",
                'Count': len(topic_docs),
                'Keywords': ', '.join([word for word, _ in keywords[:3]])
            })
    
    # Créer un DataFrame pour le graphique Sunburst
    topic_df = pd.DataFrame(topic_data)
    
    # Création du graphique Sunburst
    fig = px.sunburst(
        topic_df,
        path=['Topic'],
        values='Count',
        hover_data=['Keywords'],
        width=800,
        height=800
    )
    
    # Mise à jour du layout du graphique avec une légende
    fig.update_layout(
        title="Mapping des Topics",
        legend_title="Topic ID",
        legend=dict(
            x=0.85,
            y=0.85
        )
    )
    
    # Sauvegarde du graphique dans un fichier HTML
    fig.write_html("topic_mapping.html")
    return fig

# 6. Exécuter les analyses
analyze_topic_relationships(processed_df, topic_model)
fig = create_interactive_topic_mapping(processed_df, topic_model)
fig.show()

# 7. Sauvegarder les résultats dans un fichier Excel
detailed_analysis = pd.DataFrame({
    'topic': processed_df['topic'],
    'texte': processed_df['texte']
}).groupby('topic').agg({
    'texte': 'count'
})

detailed_analysis.to_excel('topic_analysis.xlsx')


# Section 4 : Visualisations Interactives
###########################################
# 1. Intertopic Distance Map
topic_viz = topic_model.visualize_topics()
topic_viz.show()
topic_viz.write_html("intertopic_distance_map.html")

# 2. Document Map
doc_viz = topic_model.visualize_documents(processed_df['texte'])
doc_viz.show()
doc_viz.write_html("document_visualization.html")

# 3. Topic Hierarchy
hierarchy_viz = topic_model.visualize_hierarchy()
hierarchy_viz.show()
hierarchy_viz.write_html("topic_hierarchy.html")

# 4. Topic Distribution
barchart_viz = topic_model.visualize_barchart()
barchart_viz.show()
barchart_viz.write_html("topic_distribution.html")




# Section 5 : Analyse des Relations entre Topics
###########################################

# Créer une matrice de relation entre topics
topic_matrix = processed_df[processed_df['topic'] != -1]['topic'].value_counts(normalize=True) * 100

# Visualisation des topics
plt.figure(figsize=(12, 8))
barplot = sns.barplot(x=topic_matrix.index, y=topic_matrix.values, palette='YlOrRd')

# Configuration du style
plt.gca().set_facecolor('#f0f0f0')
plt.title('Distribution des Topics par Espace')
plt.ylabel('Pourcentage (%)')
plt.xlabel('Topic ID')

# Ajouter les pourcentages sur les barres
for p in barplot.patches:
    barplot.annotate(f'{p.get_height():.2f}%', 
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    fontsize=12, color='black',
                    xytext=(0, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

# Analyse détaillée des topics
print("\nAnalyse détaillée des topics:")
for topic in sorted(processed_df['topic'].unique()):
    if topic != -1:
        print(f"\nTopic {topic}:")
        # Afficher les mots-clés
        keywords = topic_model.get_topic(topic)
        print(f"Mots-clés: {', '.join([word for word, _ in keywords[:5]])}")
        
        # Afficher les espaces associés
        print("\nExemples d'espaces dans ce topic:")
        examples = processed_df[processed_df['topic'] == topic]['nom'].head(3)
        print(examples.tolist())
        
        # Afficher un extrait de texte
        print("\nExtrait de texte exemple:")
        text_example = processed_df[processed_df['topic'] == topic]['texte'].iloc[0][:200]
        print(f"{text_example}...")


# Section 6 : Graphe des Relations Sémantiques
###########################################

# Création du graphe
G = nx.Graph()
embeddings = topic_model._extract_embeddings(processed_df['texte'].tolist())
similarities = cosine_similarity(embeddings)

# Ajouter les nœuds
for i in range(len(processed_df)):
    G.add_node(i, 
              space_name=processed_df['nom'].iloc[i],
              topic=processed_df['topic'].iloc[i])

# Ajouter les arêtes
threshold = 0.3
for i in range(len(processed_df)):
    for j in range(i+1, len(processed_df)):
        if similarities[i,j] > threshold:
            G.add_edge(i, j, weight=similarities[i,j])

# Visualisation du réseau
plt.figure(figsize=(20, 12), facecolor='white')
pos = nx.spring_layout(G, k=1, iterations=50)

# Colorer les nœuds par topic
topics = [G.nodes[n]['topic'] for n in G.nodes()]
unique_topics = list(set(topics))
colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_topics)))

# Dessiner le graphe
nx.draw(G, pos,
       node_color=[colors[unique_topics.index(t)] for t in topics],
       node_size=100,
       alpha=0.7,
       with_labels=False)

# Ajouter la légende
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                            markerfacecolor=colors[i], label=f'Topic {t}',
                            markersize=10)
                  for i, t in enumerate(unique_topics)]
plt.legend(handles=legend_elements, 
          title='Topics',
          bbox_to_anchor=(1.05, 1),
          loc='upper left')

# Titre et configuration finale
plt.title("Réseau des Relations Sémantiques entre Espaces",
         pad=20,
         fontsize=14,
         fontweight='bold')

plt.tight_layout()
plt.savefig('semantic_network.png', 
            dpi=300,
            bbox_inches='tight',
            facecolor='white')
plt.show()

# Afficher quelques statistiques sur le réseau
print("\nStatistiques du réseau:")
print(f"Nombre de nœuds: {G.number_of_nodes()}")
print(f"Nombre de connexions: {G.number_of_edges()}")
print(f"Densité du réseau: {nx.density(G):.3f}")


# Section 7 : Annotations et Pipeline
###########################################
def generate_topic_annotations(topic_model, topics):
    """
    Génère des annotations descriptives pour chaque topic
    """
    annotations = {}
    for topic_id in set(topics):
        if topic_id != -1:
            # Obtenir les mots-clés du topic
            keywords = topic_model.get_topic(topic_id)
            # Générer une annotation basée sur les mots-clés principaux
            main_keywords = [word for word, _ in keywords[:3]]
            annotation = " & ".join(main_keywords)
            annotations[topic_id] = f"Thème {topic_id}: {annotation}"
    
    # Ajouter une annotation pour le topic -1 (outliers)
    annotations[-1] = "Autres thèmes"
    return annotations

def topic_annotation_pipeline(new_texts, topic_model, existing_annotations=None):
    """
    Pipeline pour appliquer les annotations à de nouveaux textes
    
    Parameters:
    -----------
    new_texts : list
        Liste des nouveaux textes à analyser
    topic_model : BERTopic
        Modèle BERTopic entraîné
    existing_annotations : dict, optional
        Annotations existantes à réutiliser
        
    Returns:
    --------
    DataFrame avec les résultats de l'analyse
    """
    # Transformer les nouveaux textes
    new_topics, new_probs = topic_model.transform(new_texts)
    
    # Utiliser les annotations existantes ou en générer de nouvelles
    if existing_annotations is None:
        annotations = generate_topic_annotations(topic_model, new_topics)
    else:
        annotations = existing_annotations
    
    # Créer un DataFrame avec les résultats
    results = pd.DataFrame({
        'texte': new_texts,
        'topic': new_topics,
        'annotation': [annotations.get(topic, "Autre") for topic in new_topics],
        'probabilite': [np.max(prob) for prob in new_probs]
    })
    
    return results

# Application des annotations au DataFrame existant
topic_annotations = generate_topic_annotations(topic_model, topics)
processed_df['annotation'] = processed_df['topic'].map(topic_annotations)

# Affichage des résultats
print("\nExemple d'annotations générées :")
for topic_id, annotation in list(topic_annotations.items())[:5]:
    print(f"{annotation}")

# Calculer les comptes d'annotations
annotation_counts = processed_df['annotation'].value_counts()

# Créer une figure avec une taille spécifique
plt.figure(figsize=(20, 12))

# Créer le graphique en barres horizontales
bars = plt.barh(range(len(annotation_counts)), annotation_counts.values, color='skyblue', edgecolor='black')

# Configurer les étiquettes de l'axe y
plt.yticks(range(len(annotation_counts)), annotation_counts.index, fontsize=10)

# Ajouter les valeurs sur les barres
for i, bar in enumerate(bars):
    width = bar.get_width()
    plt.text(width, i, f' {int(width)}', va='center', fontsize=10, fontweight='bold')

# Configurer les titres et labels
plt.title("Distribution des Thèmes", fontsize=16, fontweight='bold', pad=20)
plt.xlabel("Nombre de Documents", fontsize=12, fontweight='bold')
plt.ylabel("Thèmes", fontsize=12, fontweight='bold')

# Ajouter une grille légère
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Ajuster la mise en page
plt.tight_layout()

# Sauvegarder et afficher
plt.savefig('distribution_themes.png', 
            dpi=300, 
            bbox_inches='tight',
            facecolor='white')
plt.show()



# Section 8 : Export des Résultats
###########################################

# Préparer le DataFrame final avec toutes les informations importantes
final_results = pd.DataFrame({
    'nom_espace': processed_df['nom'],
    'texte': processed_df['texte'],
    'topic': processed_df['topic'],
    'annotation': processed_df['annotation'],
    'probabilite': processed_df['topic_probability']
})

# Ajouter les mots-clés pour chaque espace
keywords_dict = {}
for idx, row in final_results.iterrows():
    if row['topic'] != -1:
        keywords = topic_model.get_topic(row['topic'])
        keywords_dict[idx] = ', '.join([word for word, _ in keywords[:5]])
    else:
        keywords_dict[idx] = 'Non classifié'

final_results['mots_cles'] = pd.Series(keywords_dict)

# Sauvegarder les résultats
final_results.to_excel("resultats_complets.xlsx", index=False)
final_results[['nom_espace', 'topic', 'annotation', 'mots_cles']].to_csv("resume_classification.csv", index=False)

# Afficher un résumé détaillé avec pandas
print("\n=== RÉSUMÉ DE L'ANALYSE ===")
print(f"\nNombre total d'espaces analysés : {len(final_results)}")
print(f"Nombre de topics identifiés : {len(topic_annotations)}")

print("\n=== DISTRIBUTION DES TOPICS ===")
topic_dist = final_results['topic'].value_counts().sort_index()
display(topic_dist.to_frame(name='Nombre d\'espaces'))

print("\n=== APERÇU DES RÉSULTATS PAR TOPIC ===")
for topic in sorted(final_results['topic'].unique()):
    if topic != -1:
        print(f"\nTopic {topic}:")
        subset = final_results[final_results['topic'] == topic][['nom_espace', 'annotation', 'mots_cles']]
        display(subset.head(3))
        print("-" * 100)

# Créer un résumé statistique
summary_stats = pd.DataFrame({
    'Métrique': [
        'Nombre total d\'espaces',
        'Nombre de topics',
        'Espaces non classifiés',
        'Topic le plus fréquent',
        'Taille moyenne par topic'
    ],
    'Valeur': [
        len(final_results),
        len(topic_annotations) - 1,  # -1 pour exclure le topic -1
        len(final_results[final_results['topic'] == -1]),
        f"Topic {topic_dist.index[0]} ({topic_dist.iloc[0]} espaces)",
        f"{len(final_results) / (len(topic_annotations) - 1):.1f} espaces"
    ]
})

print("\n=== STATISTIQUES GLOBALES ===")
display(summary_stats)

# Sauvegarder le modèle
import joblib
joblib.dump(topic_model, 'model_bertopic.pkl')
print("\nModèle sauvegardé sous 'model_bertopic.pkl'")




